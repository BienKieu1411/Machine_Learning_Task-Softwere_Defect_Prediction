{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ant-1.3.csv', 'ant-1.4.csv', 'ant-1.5.csv', 'ant-1.6.csv', 'ant-1.7.csv', 'arc.csv', 'camel-1.0.csv', 'camel-1.2.csv', 'camel-1.4.csv', 'camel-1.6.csv', 'ivy-1.0.csv', 'ivy-1.1.csv', 'ivy-1.2.csv', 'jedit-3.2.csv', 'jedit-4.0.csv', 'jedit-4.1.csv', 'jedit-4.2.csv', 'jedit-4.3.csv', 'log4j-1.0.csv', 'log4j-1.1.csv', 'log4j-1.2.csv', 'lucene-2.0.csv', 'lucene-2.2.csv', 'lucene-2.4.csv', 'poi-1.5.csv', 'poi-2.0.csv', 'poi-2.5.csv', 'poi-3.0.csv', 'prop-6.csv', 'redaktor.csv', 'synapse-1.0.csv', 'synapse-1.1.csv', 'synapse-1.2.csv', 'tomcat.csv', 'velocity-1.4.csv', 'velocity-1.5.csv', 'velocity-1.6.csv', 'xalan-2.4.csv', 'xalan-2.5.csv', 'xalan-2.6.csv', 'xalan-2.7.csv', 'xerces-1.1.csv', 'xerces-1.2.csv', 'xerces-1.3.csv', 'xerces-1.4.4.csv']\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"dataPROMISE/\"\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "ant_files = sorted([f for f in files if f.startswith('')])\n",
    "file_list = os.listdir(folder_path)\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ant': ['ant-1.3.csv', 'ant-1.4.csv', 'ant-1.5.csv', 'ant-1.6.csv', 'ant-1.7.csv'], 'arc': ['arc.csv'], 'camel': ['camel-1.0.csv', 'camel-1.2.csv', 'camel-1.4.csv', 'camel-1.6.csv'], 'ivy': ['ivy-1.0.csv', 'ivy-1.1.csv', 'ivy-1.2.csv'], 'jedit': ['jedit-3.2.csv', 'jedit-4.0.csv', 'jedit-4.1.csv', 'jedit-4.2.csv', 'jedit-4.3.csv'], 'log4j': ['log4j-1.0.csv', 'log4j-1.1.csv', 'log4j-1.2.csv'], 'lucene': ['lucene-2.0.csv', 'lucene-2.2.csv', 'lucene-2.4.csv'], 'poi': ['poi-1.5.csv', 'poi-2.0.csv', 'poi-2.5.csv', 'poi-3.0.csv'], 'prop': ['prop-6.csv'], 'redaktor': ['redaktor.csv'], 'synapse': ['synapse-1.0.csv', 'synapse-1.1.csv', 'synapse-1.2.csv'], 'tomcat': ['tomcat.csv'], 'velocity': ['velocity-1.4.csv', 'velocity-1.5.csv', 'velocity-1.6.csv'], 'xalan': ['xalan-2.4.csv', 'xalan-2.5.csv', 'xalan-2.6.csv', 'xalan-2.7.csv'], 'xerces': ['xerces-1.1.csv', 'xerces-1.2.csv', 'xerces-1.3.csv', 'xerces-1.4.4.csv']}\n"
     ]
    }
   ],
   "source": [
    "projects = {}\n",
    "for file in file_list:\n",
    "    project_name = \"-\".join(file.split(\"-\")[:-1])\n",
    "    if project_name == '':\n",
    "        project_name = file.split('.')[0] \n",
    "    if project_name not in projects:\n",
    "        projects[project_name] = []\n",
    "    projects[project_name].append(file)\n",
    "print(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(file_list, folder_path, index):\n",
    "    train_file = file_list[index]\n",
    "    test_file = file_list[index + 1]\n",
    "    \n",
    "    # Đọc dữ liệu\n",
    "    data_train = pd.read_csv(os.path.join(folder_path, train_file))\n",
    "    data_test = pd.read_csv(os.path.join(folder_path, test_file))       \n",
    "\n",
    "    # Tách features và labels\n",
    "    X_train, y_train = data_train.drop([\"name\", \"bug\"], axis=1), data_train[\"bug\"]\n",
    "    X_test, y_test = data_test.drop([\"name\", \"bug\"], axis=1), data_test[\"bug\"]\n",
    "\n",
    "    # Chuyển đổi nhãn thành dạng nhị phân\n",
    "    y_train = y_train.apply(lambda x: 1 if x != 0 else 0)\n",
    "    y_test = y_test.apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_test = imputer.transform(X_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def Train(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Lựa chọn đặc trưng\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "    X_train = selector.fit_transform(X_train, y_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "\n",
    "    # Xử lý mất cân bằng lớp với Borderline-SMOTE\n",
    "    smote = BorderlineSMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),  # Thêm KNN\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),  # Thêm AdaBoost\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),  # Thêm XGBoost\n",
    "        ('lgbm', LGBMClassifier(random_state=42)),  # Thêm LightGBM\n",
    "        ('nb', GaussianNB())  # Thêm Naive Bayes\n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta (sử dụng Logistic Regression)\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    # Tính toán và trả về điểm F1\n",
    "    return f1_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Without_Feature_Selection(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Xử lý mất cân bằng lớp với Borderline-SMOTE\n",
    "    smote = BorderlineSMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)),\n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta (sử dụng Logistic Regression)\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    # Tính toán và trả về điểm F1\n",
    "    return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Without_Sampling(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Lựa chọn đặc trưng\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "    X_train = selector.fit_transform(X_train, y_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)), \n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')), \n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta (sử dụng Logistic Regression)\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    # Tính toán và trả về điểm F1\n",
    "    return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Without_Weighted_Learning(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Lựa chọn đặc trưng\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "    X_train = selector.fit_transform(X_train, y_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "\n",
    "    # Xử lý mất cân bằng lớp với Borderline-SMOTE\n",
    "    smote = BorderlineSMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()), \n",
    "        ('ada', AdaBoostClassifier(random_state=42)), \n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')), \n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta (sử dụng Logistic Regression)\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, train_test_split\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def Train_Cross(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    # Xử lý dữ liệu\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype == \"object\":\n",
    "            if col == \"bug\":\n",
    "                dataset[col] = dataset[col].astype(float)\n",
    "            else:\n",
    "                dataset = dataset.drop([col], axis=1)\n",
    "\n",
    "    X = dataset.drop([\"bug\"], axis=1)\n",
    "    y = dataset[\"bug\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    # Tạo pipeline cho việc tiền xử lý dữ liệu\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=mutual_info_classif)),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation với SMOTE chỉ áp dụng lên tập huấn luyện\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    \n",
    "    f1_scores = []  # Lưu trữ điểm F1 của mỗi lần phân chia\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Áp dụng SMOTE chỉ trên tập huấn luyện\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Biến đổi dữ liệu qua pipeline\n",
    "        X_train_transformed = pipeline.fit_transform(X_train_resampled, y_train_resampled)\n",
    "        X_test_transformed = pipeline.transform(X_test)  # Chỉ biến đổi dữ liệu test\n",
    "\n",
    "        # Định nghĩa các mô hình cơ sở\n",
    "        base_models = [\n",
    "            ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "            ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "            ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('knn', KNeighborsClassifier()),\n",
    "            ('ada', AdaBoostClassifier(random_state=42)),\n",
    "            ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "            ('lgbm', LGBMClassifier(random_state=42)),\n",
    "            ('nb', GaussianNB())\n",
    "        ]\n",
    "\n",
    "        meta_model = LogisticRegression()\n",
    "\n",
    "        # Sử dụng StackingClassifier\n",
    "        stacking_clf = StackingClassifier(\n",
    "            estimators=base_models, \n",
    "            final_estimator=meta_model,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Huấn luyện mô hình trên tập huấn luyện đã resample\n",
    "        stacking_clf.fit(X_train_transformed, y_train_resampled)\n",
    "\n",
    "        # Dự đoán trên tập kiểm tra\n",
    "        from sklearn.metrics import f1_score\n",
    "        y_pred = stacking_clf.predict(X_test_transformed)\n",
    "        \n",
    "        # Tính toán điểm F1 trên tập kiểm tra\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Trả về điểm F1 trung bình qua các vòng cross-validation\n",
    "    return sum(f1_scores) / len(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cross_Without_Feature_Selection(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype == \"object\":\n",
    "            if col == \"bug\":\n",
    "                dataset[col] = dataset[col].astype(float)\n",
    "            else:\n",
    "                dataset = dataset.drop([col], axis=1)\n",
    "    X = dataset.drop([\"bug\"], axis=1)\n",
    "    y = dataset[\"bug\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Tạo pipeline cho dữ liệu đã resample\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Biến đổi dữ liệu qua pipeline\n",
    "    X_transformed = pipeline.fit_transform(X_resampled, y_resampled)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB())\n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier \n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    scores = cross_val_score(stacking_clf, X_transformed, y_resampled, cv=kf, scoring='f1', n_jobs=-1)\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cross_Without_Sampling(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype == \"object\":\n",
    "            if col == \"bug\":\n",
    "                dataset[col] = dataset[col].astype(float)\n",
    "            else:\n",
    "                dataset = dataset.drop([col], axis=1)\n",
    "    X = dataset.drop([\"bug\"], axis=1)\n",
    "    y = dataset[\"bug\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "    \n",
    "    # Tạo pipeline cho dữ liệu đã resample\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=mutual_info_classif)),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Biến đổi dữ liệu qua pipeline\n",
    "    X_transformed = pipeline.fit_transform(X, y)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB())\n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier \n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    scores = cross_val_score(stacking_clf, X_transformed, y, cv=kf, scoring='f1', n_jobs=-1)\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cross_Without_Weighted_Learning(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype == \"object\":\n",
    "            if col == \"bug\":\n",
    "                dataset[col] = dataset[col].astype(float)\n",
    "            else:\n",
    "                dataset = dataset.drop([col], axis=1)\n",
    "    X = dataset.drop([\"bug\"], axis=1)\n",
    "    y = dataset[\"bug\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Tạo pipeline cho dữ liệu đã resample\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=mutual_info_classif)),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Biến đổi dữ liệu qua pipeline\n",
    "    X_transformed = pipeline.fit_transform(X_resampled, y_resampled)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()), \n",
    "        ('ada', AdaBoostClassifier(random_state=42)), \n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')), \n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier \n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    scores = cross_val_score(stacking_clf, X_transformed, y_resampled, cv=kf, scoring='f1', n_jobs=-1)\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results = []\n",
    "results_without_feature_selection = []\n",
    "results_without_sampling = []\n",
    "results_without_weighted_learning = []\n",
    "for project, versions in projects.items():\n",
    "    project_scores = []\n",
    "    project_scores_without_feature_selection = []\n",
    "    project_scores_without_sampling = []\n",
    "    project_scores_without_weighted_learning = []\n",
    "    if len(versions) != 1:\n",
    "        for i in range(len(versions) - 1):\n",
    "            project_scores.append(Train(versions, folder_path, i))\n",
    "            project_scores_without_feature_selection.append(Train_Without_Feature_Selection(versions, folder_path, i))\n",
    "            project_scores_without_sampling.append(Train_Without_Sampling(versions, folder_path, i))\n",
    "            project_scores_without_weighted_learning.append(Train_Without_Weighted_Learning(versions, folder_path, i))\n",
    "\n",
    "        avg_score = np.mean(project_scores) \n",
    "        avg_score_without_feature_selection = np.mean(project_scores_without_feature_selection)\n",
    "        avg_score_without_sampling = np.mean(project_scores_without_sampling)\n",
    "        avg_score_without_weighted_learning = np.mean(project_scores_without_weighted_learning)\n",
    "\n",
    "        results.append({\"Project\": project, \"F1_Score\": avg_score})\n",
    "        results_without_feature_selection.append({\"Project\": project, \"F1_Score\": avg_score_without_feature_selection})\n",
    "        results_without_sampling.append({\"Project\": project, \"F1_Score\": avg_score_without_sampling})\n",
    "        results_without_weighted_learning.append({\"Project\": project, \"F1_Score\": avg_score_without_weighted_learning})\n",
    "    else:\n",
    "        avg_score = Train_Cross(versions, folder_path)\n",
    "        avg_score_without_feature_selection = Train_Cross_Without_Feature_Selection(versions, folder_path)\n",
    "        avg_score_without_sampling = Train_Cross_Without_Sampling(versions, folder_path)\n",
    "        avg_score_without_weighted_learning = Train_Cross_Without_Weighted_Learning(versions, folder_path)\n",
    "        \n",
    "        results.append({\"Project\": project, \"F1_Score\": avg_score})\n",
    "        results_without_feature_selection.append({\"Project\": project, \"F1_Score\": avg_score_without_feature_selection})\n",
    "        results_without_sampling.append({\"Project\": project, \"F1_Score\": avg_score_without_sampling})\n",
    "        results_without_weighted_learning.append({\"Project\": project, \"F1_Score\": avg_score_without_weighted_learning})\n",
    "\n",
    "avg_results = np.mean([result[\"F1_Score\"] for result in results])\n",
    "avg_results_without_feature_selection = np.mean([result[\"F1_Score\"] for result in results_without_feature_selection])\n",
    "avg_results_without_sampling = np.mean([result[\"F1_Score\"] for result in results_without_sampling])\n",
    "avg_results_without_weighted_learning = np.mean([result[\"F1_Score\"] for result in results_without_weighted_learning])\n",
    "\n",
    "results.append({\"Project\": \"Avg\", \"F1_Score\": avg_results})\n",
    "results_without_feature_selection.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_feature_selection})\n",
    "results_without_sampling.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_sampling})\n",
    "results_without_weighted_learning.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_weighted_learning})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results_without_feature_selection = pd.DataFrame(results_without_feature_selection)\n",
    "df_results_without_sampling = pd.DataFrame(results_without_sampling)\n",
    "df_results_without_weighted_learning = pd.DataFrame(results_without_weighted_learning)\n",
    "\n",
    "output_file = 'dataPROMISE_results.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_results.to_excel(writer, sheet_name='Results', index=False)\n",
    "    df_results_without_feature_selection.to_excel(writer, sheet_name='Results_Without_Feature_Selection', index=False)\n",
    "    df_results_without_sampling.to_excel(writer, sheet_name='Results_Without_Sampling', index=False)\n",
    "    df_results_without_weighted_learning.to_excel(writer, sheet_name='Results_Without_Weighted_Learning', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CM1.csv', 'KC1.csv', 'KC3.csv', 'MC1.csv', 'MC2.csv', 'MW1.csv', 'PC1.csv', 'PC3.csv', 'PC4.csv', 'PC5.csv']\n"
     ]
    }
   ],
   "source": [
    "folder_path_nasa = \"dataNASA/\"\n",
    "files_nasa = [f for f in os.listdir(folder_path_nasa) if f.endswith('.csv')]\n",
    "ant_files_nasa = sorted([f for f in files if f.startswith('')])\n",
    "file_list_nasa = os.listdir(folder_path_nasa)\n",
    "print(file_list_nasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results_without_feature_selection = []\n",
    "results_without_sampling = []\n",
    "results_without_weighted_learning = []\n",
    "for file in file_list_nasa:\n",
    "        avg_score = Train_Cross(versions, folder_path)\n",
    "        avg_score_without_feature_selection = Train_Cross_Without_Feature_Selection(versions, folder_path)\n",
    "        avg_score_without_sampling = Train_Cross_Without_Sampling(versions, folder_path)\n",
    "        avg_score_without_weighted_learning = Train_Cross_Without_Weighted_Learning(versions, folder_path)\n",
    "        \n",
    "        results.append({\"Project\": file, \"F1_Score\": avg_score})\n",
    "        results_without_feature_selection.append({\"Project\": file, \"F1_Score\": avg_score_without_feature_selection})\n",
    "        results_without_sampling.append({\"Project\": file, \"F1_Score\": avg_score_without_sampling})\n",
    "        results_without_weighted_learning.append({\"Project\": file, \"F1_Score\": avg_score_without_weighted_learning})\n",
    "\n",
    "avg_results = np.mean([result[\"F1_Score\"] for result in results])\n",
    "avg_results_without_feature_selection = np.mean([result[\"F1_Score\"] for result in results_without_feature_selection])\n",
    "avg_results_without_sampling = np.mean([result[\"F1_Score\"] for result in results_without_sampling])\n",
    "avg_results_without_weighted_learning = np.mean([result[\"F1_Score\"] for result in results_without_weighted_learning])\n",
    "\n",
    "results.append({\"Project\": \"Avg\", \"F1_Score\": avg_results})\n",
    "results_without_feature_selection.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_feature_selection})\n",
    "results_without_sampling.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_sampling})\n",
    "results_without_weighted_learning.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_weighted_learning})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results_without_feature_selection = pd.DataFrame(results_without_feature_selection)\n",
    "df_results_without_sampling = pd.DataFrame(results_without_sampling)\n",
    "df_results_without_weighted_learning = pd.DataFrame(results_without_weighted_learning)\n",
    "\n",
    "output_file = 'dataNASA_results.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_results.to_excel(writer, sheet_name='Results', index=False)\n",
    "    df_results_without_feature_selection.to_excel(writer, sheet_name='Results_Without_Feature_Selection', index=False)\n",
    "    df_results_without_sampling.to_excel(writer, sheet_name='Results_Without_Sampling', index=False)\n",
    "    df_results_without_weighted_learning.to_excel(writer, sheet_name='Results_Without_Weighted_Learning', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
