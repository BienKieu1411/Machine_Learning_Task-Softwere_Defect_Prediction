{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ant-1.3.csv', 'ant-1.4.csv', 'ant-1.5.csv', 'ant-1.6.csv', 'ant-1.7.csv', 'arc.csv', 'camel-1.0.csv', 'camel-1.2.csv', 'camel-1.4.csv', 'camel-1.6.csv', 'ivy-1.0.csv', 'ivy-1.1.csv', 'ivy-1.2.csv', 'jedit-3.2.csv', 'jedit-4.0.csv', 'jedit-4.1.csv', 'jedit-4.2.csv', 'jedit-4.3.csv', 'log4j-1.0.csv', 'log4j-1.1.csv', 'log4j-1.2.csv', 'lucene-2.0.csv', 'lucene-2.2.csv', 'lucene-2.4.csv', 'poi-1.5.csv', 'poi-2.0.csv', 'poi-2.5.csv', 'poi-3.0.csv', 'prop-6.csv', 'redaktor.csv', 'synapse-1.0.csv', 'synapse-1.1.csv', 'synapse-1.2.csv', 'tomcat.csv', 'velocity-1.4.csv', 'velocity-1.5.csv', 'velocity-1.6.csv', 'xalan-2.4.csv', 'xalan-2.5.csv', 'xalan-2.6.csv', 'xalan-2.7.csv', 'xerces-1.1.csv', 'xerces-1.2.csv', 'xerces-1.3.csv', 'xerces-1.4.4.csv']\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"dataPROMISE/\"\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "ant_files = sorted([f for f in files if f.startswith('')])\n",
    "file_list = os.listdir(folder_path)\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ant': ['ant-1.3.csv', 'ant-1.4.csv', 'ant-1.5.csv', 'ant-1.6.csv', 'ant-1.7.csv'], 'arc': ['arc.csv'], 'camel': ['camel-1.0.csv', 'camel-1.2.csv', 'camel-1.4.csv', 'camel-1.6.csv'], 'ivy': ['ivy-1.0.csv', 'ivy-1.1.csv', 'ivy-1.2.csv'], 'jedit': ['jedit-3.2.csv', 'jedit-4.0.csv', 'jedit-4.1.csv', 'jedit-4.2.csv', 'jedit-4.3.csv'], 'log4j': ['log4j-1.0.csv', 'log4j-1.1.csv', 'log4j-1.2.csv'], 'lucene': ['lucene-2.0.csv', 'lucene-2.2.csv', 'lucene-2.4.csv'], 'poi': ['poi-1.5.csv', 'poi-2.0.csv', 'poi-2.5.csv', 'poi-3.0.csv'], 'prop': ['prop-6.csv'], 'redaktor': ['redaktor.csv'], 'synapse': ['synapse-1.0.csv', 'synapse-1.1.csv', 'synapse-1.2.csv'], 'tomcat': ['tomcat.csv'], 'velocity': ['velocity-1.4.csv', 'velocity-1.5.csv', 'velocity-1.6.csv'], 'xalan': ['xalan-2.4.csv', 'xalan-2.5.csv', 'xalan-2.6.csv', 'xalan-2.7.csv'], 'xerces': ['xerces-1.1.csv', 'xerces-1.2.csv', 'xerces-1.3.csv', 'xerces-1.4.4.csv']}\n"
     ]
    }
   ],
   "source": [
    "projects = {}\n",
    "for file in file_list:\n",
    "    project_name = \"-\".join(file.split(\"-\")[:-1])\n",
    "    if project_name == '':\n",
    "        project_name = file.split('.')[0] \n",
    "    if project_name not in projects:\n",
    "        projects[project_name] = []\n",
    "    projects[project_name].append(file)\n",
    "print(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'wmc', 'dit', 'noc', 'cbo', 'rfc', 'lcom', 'ca', 'ce', 'npm',\n",
       "       'lcom3', 'loc', 'dam', 'moa', 'mfa', 'cam', 'ic', 'cbm', 'amc',\n",
       "       'max_cc', 'avg_cc', 'bug'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(folder_path, file_list[0]))\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(file_list, folder_path, index):\n",
    "    train_file = file_list[index]\n",
    "    test_file = file_list[index + 1]\n",
    "    \n",
    "    # Đọc dữ liệu\n",
    "    data_train = pd.read_csv(os.path.join(folder_path, train_file))\n",
    "    data_test = pd.read_csv(os.path.join(folder_path, test_file))       \n",
    "\n",
    "    # Tách features và labels\n",
    "    X_train, y_train = data_train.drop([\"name\", \"bug\"], axis=1), data_train[\"bug\"]\n",
    "    X_test, y_test = data_test.drop([\"name\", \"bug\"], axis=1), data_test[\"bug\"]\n",
    "\n",
    "    # Chuyển đổi nhãn thành dạng nhị phân\n",
    "    y_train = y_train.apply(lambda x: 1 if x != 0 else 0)\n",
    "    y_test = y_test.apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_test = imputer.transform(X_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_cross(dataset, folder_path):\n",
    "    if(folder_path == \"dataPROMISE/\"):\n",
    "        for col in dataset.columns:\n",
    "            if dataset[col].dtype == \"object\":\n",
    "                if col == \"bug\":\n",
    "                    dataset[col] = dataset[col].astype(float)\n",
    "                else:\n",
    "                    dataset = dataset.drop([col], axis=1)\n",
    "\n",
    "        X = dataset.drop([\"bug\"], axis=1)\n",
    "        y = dataset[\"bug\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "    if(folder_path == \"dataNASA/\"):\n",
    "        onehot = {\n",
    "            \"N\" : 0,\n",
    "            \"Y\" : 1\n",
    "        }\n",
    "        dataset[\"Defective\"] = dataset[\"Defective\"].map(onehot)\n",
    "        X = dataset.drop([\"Defective\"], axis= 1)\n",
    "        y = dataset[\"Defective\"]\n",
    "    return  X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Lựa chọn đặc trưng\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "    X_train = selector.fit_transform(X_train, y_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "\n",
    "    # Xử lý mất cân bằng với Borderline-SMOTE\n",
    "    smote = BorderlineSMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()), \n",
    "        ('ada', AdaBoostClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')), \n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta \n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy, f1, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Without_Feature_Selection(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Xử lý mất cân bằng lớp với Borderline-SMOTE\n",
    "    smote = BorderlineSMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)),\n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta (sử dụng Logistic Regression)\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy, f1, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Without_Sampling(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Lựa chọn đặc trưng\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "    X_train = selector.fit_transform(X_train, y_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)), \n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')), \n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta \n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy, f1, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Without_Weighted_Learning(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Lựa chọn đặc trưng\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "    X_train = selector.fit_transform(X_train, y_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "\n",
    "    # Xử lý mất cân bằng lớp với Borderline-SMOTE\n",
    "    smote = BorderlineSMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()), \n",
    "        ('ada', AdaBoostClassifier(random_state=42)), \n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')), \n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta \n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy, f1, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cross(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    # Xử lý dữ liệu\n",
    "    X, y = pre_processing_cross(dataset, folder_path)\n",
    "\n",
    "    # Áp dụng SMOTE để cân bằng dữ liệu\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Pipeline tiền xử lý dữ liệu\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=mutual_info_classif)),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Biến đổi dữ liệu\n",
    "    X_transformed = pipeline.fit_transform(X_resampled, y_resampled)\n",
    "\n",
    "    # Định nghĩa các mô hình cơ sở\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)),\n",
    "        ('nb', GaussianNB())\n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score),\n",
    "        'recall': make_scorer(recall_score)\n",
    "    }\n",
    "\n",
    "    # Tính các chỉ số đánh giá\n",
    "    scores = cross_validate(stacking_clf, X_transformed, y_resampled, cv=kf, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "    acc_mean = scores['test_accuracy'].mean()\n",
    "    f1_mean = scores['test_f1'].mean()\n",
    "    recall_mean = scores['test_recall'].mean()\n",
    "\n",
    "    return acc_mean, f1_mean, recall_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cross_Without_Feature_Selection(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    # Xử lý dữ liệu\n",
    "    X, y = pre_processing_cross(dataset, folder_path)\n",
    "\n",
    "\n",
    "    # Cân bằng dữ liệu bằng SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Tạo pipeline tiền xử lý\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Biến đổi dữ liệu qua pipeline\n",
    "    X_transformed = pipeline.fit_transform(X_resampled, y_resampled)\n",
    "\n",
    "    # Định nghĩa các mô hình base\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB())\n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Stacking Classifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score),\n",
    "        'recall': make_scorer(recall_score)\n",
    "    }\n",
    "\n",
    "    scores = cross_validate(stacking_clf, X_transformed, y_resampled, cv=kf, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "    acc_mean = scores['test_accuracy'].mean()\n",
    "    f1_mean = scores['test_f1'].mean()\n",
    "    recall_mean = scores['test_recall'].mean()\n",
    "\n",
    "    return acc_mean, f1_mean, recall_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cross_Without_Sampling(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    # Xử lý dữ liệu\n",
    "    X, y = pre_processing_cross(dataset, folder_path)\n",
    "    \n",
    "    # Tạo pipeline tiền xử lý dữ liệu\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=mutual_info_classif)),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Biến đổi dữ liệu qua pipeline\n",
    "    X_transformed = pipeline.fit_transform(X, y)\n",
    "\n",
    "    # Định nghĩa các mô hình cơ sở\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB())\n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier \n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Cross-validation setup\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score),\n",
    "        'recall': make_scorer(recall_score)\n",
    "    }\n",
    "\n",
    "    # Tính các chỉ số đánh giá\n",
    "    scores = cross_validate(stacking_clf, X_transformed, y, cv=kf, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "    acc_mean = scores['test_accuracy'].mean()\n",
    "    f1_mean = scores['test_f1'].mean()\n",
    "    recall_mean = scores['test_recall'].mean()\n",
    "\n",
    "    return acc_mean, f1_mean, recall_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cross_Without_Weighted_Learning(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    # Xử lý dữ liệu\n",
    "    X, y = pre_processing_cross(dataset, folder_path)\n",
    "\n",
    "    # Áp dụng SMOTE để cân bằng dữ liệu\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Tạo pipeline tiền xử lý dữ liệu\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=mutual_info_classif)),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Biến đổi dữ liệu qua pipeline\n",
    "    X_transformed = pipeline.fit_transform(X_resampled, y_resampled)\n",
    "\n",
    "    # Định nghĩa các mô hình cơ sở (bỏ class_weight để tránh học có trọng số)\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()), \n",
    "        ('ada', AdaBoostClassifier(random_state=42)), \n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')), \n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier \n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Cross-validation setup\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1': make_scorer(f1_score),\n",
    "        'recall': make_scorer(recall_score)\n",
    "    }\n",
    "\n",
    "    # Tính các chỉ số đánh giá\n",
    "    scores = cross_validate(stacking_clf, X_transformed, y_resampled, cv=kf, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "    acc_mean = scores['test_accuracy'].mean()\n",
    "    f1_mean = scores['test_f1'].mean()\n",
    "    recall_mean = scores['test_recall'].mean()\n",
    "\n",
    "    return acc_mean, f1_mean, recall_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results_without_feature_selection = []\n",
    "results_without_sampling = []\n",
    "results_without_weighted_learning = []\n",
    "\n",
    "for project, versions in projects.items():\n",
    "    project_scores = {\"accuracy\": [], \"f1\": [], \"recall\": []}\n",
    "    project_scores_without_feature_selection = {\"accuracy\": [], \"f1\": [], \"recall\": []}\n",
    "    project_scores_without_sampling = {\"accuracy\": [], \"f1\": [], \"recall\": []}\n",
    "    project_scores_without_weighted_learning = {\"accuracy\": [], \"f1\": [], \"recall\": []}\n",
    "\n",
    "    if len(versions) != 1:\n",
    "        for i in range(len(versions) - 1):\n",
    "            acc, f1, rec = Train(versions, folder_path, i)\n",
    "            project_scores[\"accuracy\"].append(acc)\n",
    "            project_scores[\"f1\"].append(f1)\n",
    "            project_scores[\"recall\"].append(rec)\n",
    "\n",
    "            acc, f1, rec = Train_Without_Feature_Selection(versions, folder_path, i)\n",
    "            project_scores_without_feature_selection[\"accuracy\"].append(acc)\n",
    "            project_scores_without_feature_selection[\"f1\"].append(f1)\n",
    "            project_scores_without_feature_selection[\"recall\"].append(rec)\n",
    "\n",
    "            acc, f1, rec = Train_Without_Sampling(versions, folder_path, i)\n",
    "            project_scores_without_sampling[\"accuracy\"].append(acc)\n",
    "            project_scores_without_sampling[\"f1\"].append(f1)\n",
    "            project_scores_without_sampling[\"recall\"].append(rec)\n",
    "\n",
    "            acc, f1, rec = Train_Without_Weighted_Learning(versions, folder_path, i)\n",
    "            project_scores_without_weighted_learning[\"accuracy\"].append(acc)\n",
    "            project_scores_without_weighted_learning[\"f1\"].append(f1)\n",
    "            project_scores_without_weighted_learning[\"recall\"].append(rec)\n",
    "\n",
    "        avg_scores = {key: np.mean(values) for key, values in project_scores.items()}\n",
    "        avg_scores_without_feature_selection = {key: np.mean(values) for key, values in project_scores_without_feature_selection.items()}\n",
    "        avg_scores_without_sampling = {key: np.mean(values) for key, values in project_scores_without_sampling.items()}\n",
    "        avg_scores_without_weighted_learning = {key: np.mean(values) for key, values in project_scores_without_weighted_learning.items()}\n",
    "\n",
    "    else:\n",
    "        avg_scores = dict(zip([\"accuracy\", \"f1\", \"recall\"], Train_Cross(versions, folder_path)))\n",
    "        avg_scores_without_feature_selection = dict(zip([\"accuracy\", \"f1\", \"recall\"], Train_Cross_Without_Feature_Selection(versions, folder_path)))\n",
    "        avg_scores_without_sampling = dict(zip([\"accuracy\", \"f1\", \"recall\"], Train_Cross_Without_Sampling(versions, folder_path)))\n",
    "        avg_scores_without_weighted_learning = dict(zip([\"accuracy\", \"f1\", \"recall\"], Train_Cross_Without_Weighted_Learning(versions, folder_path)))\n",
    "\n",
    "    results.append({\"Project\": project, **avg_scores})\n",
    "    results_without_feature_selection.append({\"Project\": project, **avg_scores_without_feature_selection})\n",
    "    results_without_sampling.append({\"Project\": project, **avg_scores_without_sampling})\n",
    "    results_without_weighted_learning.append({\"Project\": project, **avg_scores_without_weighted_learning})\n",
    "\n",
    "avg_results = {key: np.mean([r[key] for r in results]) for key in [\"accuracy\", \"f1\", \"recall\"]}\n",
    "avg_results_without_feature_selection = {key: np.mean([r[key] for r in results_without_feature_selection]) for key in [\"accuracy\", \"f1\", \"recall\"]}\n",
    "avg_results_without_sampling = {key: np.mean([r[key] for r in results_without_sampling]) for key in [\"accuracy\", \"f1\", \"recall\"]}\n",
    "avg_results_without_weighted_learning = {key: np.mean([r[key] for r in results_without_weighted_learning]) for key in [\"accuracy\", \"f1\", \"recall\"]}\n",
    "\n",
    "results.append({\"Project\": \"Avg\", **avg_results})\n",
    "results_without_feature_selection.append({\"Project\": \"Avg\", **avg_results_without_feature_selection})\n",
    "results_without_sampling.append({\"Project\": \"Avg\", **avg_results_without_sampling})\n",
    "results_without_weighted_learning.append({\"Project\": \"Avg\", **avg_results_without_weighted_learning})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPTOP PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results_without_feature_selection = pd.DataFrame(results_without_feature_selection)\n",
    "df_results_without_sampling = pd.DataFrame(results_without_sampling)\n",
    "df_results_without_weighted_learning = pd.DataFrame(results_without_weighted_learning)\n",
    "\n",
    "output_file = 'dataPROMISE_results.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_results.to_excel(writer, sheet_name='Results', index=False)\n",
    "    df_results_without_feature_selection.to_excel(writer, sheet_name='Results_Without_Feature_Selection', index=False)\n",
    "    df_results_without_sampling.to_excel(writer, sheet_name='Results_Without_Sampling', index=False)\n",
    "    df_results_without_weighted_learning.to_excel(writer, sheet_name='Results_Without_Weighted_Learning', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CM1.csv', 'KC1.csv', 'KC3.csv', 'MC1.csv', 'MC2.csv', 'MW1.csv', 'PC1.csv', 'PC3.csv', 'PC4.csv', 'PC5.csv']\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"dataNASA/\"\n",
    "file_list_nasa = sorted([f for f in os.listdir(folder_path) if f.endswith('.csv')])\n",
    "print(file_list_nasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'LOC_BLANK', 'BRANCH_COUNT', 'CALL_PAIRS', 'LOC_CODE_AND_COMMENT',\n",
       "       'LOC_COMMENTS', 'CONDITION_COUNT', 'CYCLOMATIC_COMPLEXITY',\n",
       "       'CYCLOMATIC_DENSITY', 'DECISION_COUNT', 'DECISION_DENSITY',\n",
       "       'DESIGN_COMPLEXITY', 'DESIGN_DENSITY', 'EDGE_COUNT',\n",
       "       'ESSENTIAL_COMPLEXITY', 'ESSENTIAL_DENSITY', 'LOC_EXECUTABLE',\n",
       "       'PARAMETER_COUNT', 'HALSTEAD_CONTENT', 'HALSTEAD_DIFFICULTY',\n",
       "       'HALSTEAD_EFFORT', 'HALSTEAD_ERROR_EST', 'HALSTEAD_LENGTH',\n",
       "       'HALSTEAD_LEVEL', 'HALSTEAD_PROG_TIME', 'HALSTEAD_VOLUME',\n",
       "       'MAINTENANCE_SEVERITY', 'MODIFIED_CONDITION_COUNT',\n",
       "       'MULTIPLE_CONDITION_COUNT', 'NODE_COUNT',\n",
       "       'NORMALIZED_CYLOMATIC_COMPLEXITY', 'NUM_OPERANDS', 'NUM_OPERATORS',\n",
       "       'NUM_UNIQUE_OPERANDS', 'NUM_UNIQUE_OPERATORS', 'NUMBER_OF_LINES',\n",
       "       'PERCENT_COMMENTS', 'LOC_TOTAL', 'Defective'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(folder_path, file_list_nasa[0]))\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results_without_feature_selection = []\n",
    "results_without_sampling = []\n",
    "results_without_weighted_learning = []\n",
    "\n",
    "for file in file_list_nasa:\n",
    "    acc, f1, rec = Train_Cross([file], folder_path)\n",
    "    acc_no_fs, f1_no_fs, rec_no_fs = Train_Cross_Without_Feature_Selection([file], folder_path)\n",
    "    acc_no_samp, f1_no_samp, rec_no_samp = Train_Cross_Without_Sampling([file], folder_path)\n",
    "    acc_no_weight, f1_no_weight, rec_no_weight = Train_Cross_Without_Weighted_Learning([file], folder_path)\n",
    "    \n",
    "    results.append({\"Project\": file, \"Accuracy\": acc, \"F1_Score\": f1, \"Recall\": rec})\n",
    "    results_without_feature_selection.append({\"Project\": file, \"Accuracy\": acc_no_fs, \"F1_Score\": f1_no_fs, \"Recall\": rec_no_fs})\n",
    "    results_without_sampling.append({\"Project\": file, \"Accuracy\": acc_no_samp, \"F1_Score\": f1_no_samp, \"Recall\": rec_no_samp})\n",
    "    results_without_weighted_learning.append({\"Project\": file, \"Accuracy\": acc_no_weight, \"F1_Score\": f1_no_weight, \"Recall\": rec_no_weight})\n",
    "\n",
    "avg_acc = np.mean([result[\"Accuracy\"] for result in results])\n",
    "avg_f1 = np.mean([result[\"F1_Score\"] for result in results])\n",
    "avg_rec = np.mean([result[\"Recall\"] for result in results])\n",
    "\n",
    "avg_acc_no_fs = np.mean([result[\"Accuracy\"] for result in results_without_feature_selection])\n",
    "avg_f1_no_fs = np.mean([result[\"F1_Score\"] for result in results_without_feature_selection])\n",
    "avg_rec_no_fs = np.mean([result[\"Recall\"] for result in results_without_feature_selection])\n",
    "\n",
    "avg_acc_no_samp = np.mean([result[\"Accuracy\"] for result in results_without_sampling])\n",
    "avg_f1_no_samp = np.mean([result[\"F1_Score\"] for result in results_without_sampling])\n",
    "avg_rec_no_samp = np.mean([result[\"Recall\"] for result in results_without_sampling])\n",
    "\n",
    "avg_acc_no_weight = np.mean([result[\"Accuracy\"] for result in results_without_weighted_learning])\n",
    "avg_f1_no_weight = np.mean([result[\"F1_Score\"] for result in results_without_weighted_learning])\n",
    "avg_rec_no_weight = np.mean([result[\"Recall\"] for result in results_without_weighted_learning])\n",
    "\n",
    "results.append({\"Project\": \"Avg\", \"Accuracy\": avg_acc, \"F1_Score\": avg_f1, \"Recall\": avg_rec})\n",
    "results_without_feature_selection.append({\"Project\": \"Avg\", \"Accuracy\": avg_acc_no_fs, \"F1_Score\": avg_f1_no_fs, \"Recall\": avg_rec_no_fs})\n",
    "results_without_sampling.append({\"Project\": \"Avg\", \"Accuracy\": avg_acc_no_samp, \"F1_Score\": avg_f1_no_samp, \"Recall\": avg_rec_no_samp})\n",
    "results_without_weighted_learning.append({\"Project\": \"Avg\", \"Accuracy\": avg_acc_no_weight, \"F1_Score\": avg_f1_no_weight, \"Recall\": avg_rec_no_weight})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPTOP PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results_without_feature_selection = pd.DataFrame(results_without_feature_selection)\n",
    "df_results_without_sampling = pd.DataFrame(results_without_sampling)\n",
    "df_results_without_weighted_learning = pd.DataFrame(results_without_weighted_learning)\n",
    "\n",
    "output_file = 'dataNASA_results.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_results.to_excel(writer, sheet_name='Results', index=False)\n",
    "    df_results_without_feature_selection.to_excel(writer, sheet_name='Results_Without_Feature_Selection', index=False)\n",
    "    df_results_without_sampling.to_excel(writer, sheet_name='Results_Without_Sampling', index=False)\n",
    "    df_results_without_weighted_learning.to_excel(writer, sheet_name='Results_Without_Weighted_Learning', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
