{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ant-1.3.csv', 'ant-1.4.csv', 'ant-1.5.csv', 'ant-1.6.csv', 'ant-1.7.csv', 'arc.csv', 'camel-1.0.csv', 'camel-1.2.csv', 'camel-1.4.csv', 'camel-1.6.csv', 'ivy-1.0.csv', 'ivy-1.1.csv', 'ivy-1.2.csv', 'jedit-3.2.csv', 'jedit-4.0.csv', 'jedit-4.1.csv', 'jedit-4.2.csv', 'jedit-4.3.csv', 'log4j-1.0.csv', 'log4j-1.1.csv', 'log4j-1.2.csv', 'lucene-2.0.csv', 'lucene-2.2.csv', 'lucene-2.4.csv', 'poi-1.5.csv', 'poi-2.0.csv', 'poi-2.5.csv', 'poi-3.0.csv', 'prop-6.csv', 'redaktor.csv', 'synapse-1.0.csv', 'synapse-1.1.csv', 'synapse-1.2.csv', 'tomcat.csv', 'velocity-1.4.csv', 'velocity-1.5.csv', 'velocity-1.6.csv', 'xalan-2.4.csv', 'xalan-2.5.csv', 'xalan-2.6.csv', 'xalan-2.7.csv', 'xerces-1.1.csv', 'xerces-1.2.csv', 'xerces-1.3.csv', 'xerces-1.4.4.csv']\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"dataPROMISE/\"\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "ant_files = sorted([f for f in files if f.startswith('')])\n",
    "file_list = os.listdir(folder_path)\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ant': ['ant-1.3.csv', 'ant-1.4.csv', 'ant-1.5.csv', 'ant-1.6.csv', 'ant-1.7.csv'], 'arc': ['arc.csv'], 'camel': ['camel-1.0.csv', 'camel-1.2.csv', 'camel-1.4.csv', 'camel-1.6.csv'], 'ivy': ['ivy-1.0.csv', 'ivy-1.1.csv', 'ivy-1.2.csv'], 'jedit': ['jedit-3.2.csv', 'jedit-4.0.csv', 'jedit-4.1.csv', 'jedit-4.2.csv', 'jedit-4.3.csv'], 'log4j': ['log4j-1.0.csv', 'log4j-1.1.csv', 'log4j-1.2.csv'], 'lucene': ['lucene-2.0.csv', 'lucene-2.2.csv', 'lucene-2.4.csv'], 'poi': ['poi-1.5.csv', 'poi-2.0.csv', 'poi-2.5.csv', 'poi-3.0.csv'], 'prop': ['prop-6.csv'], 'redaktor': ['redaktor.csv'], 'synapse': ['synapse-1.0.csv', 'synapse-1.1.csv', 'synapse-1.2.csv'], 'tomcat': ['tomcat.csv'], 'velocity': ['velocity-1.4.csv', 'velocity-1.5.csv', 'velocity-1.6.csv'], 'xalan': ['xalan-2.4.csv', 'xalan-2.5.csv', 'xalan-2.6.csv', 'xalan-2.7.csv'], 'xerces': ['xerces-1.1.csv', 'xerces-1.2.csv', 'xerces-1.3.csv', 'xerces-1.4.4.csv']}\n"
     ]
    }
   ],
   "source": [
    "projects = {}\n",
    "for file in file_list:\n",
    "    project_name = \"-\".join(file.split(\"-\")[:-1])\n",
    "    if project_name == '':\n",
    "        project_name = file.split('.')[0] \n",
    "    if project_name not in projects:\n",
    "        projects[project_name] = []\n",
    "    projects[project_name].append(file)\n",
    "print(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(file_list, folder_path, index):\n",
    "    train_file = file_list[index]\n",
    "    test_file = file_list[index + 1]\n",
    "    \n",
    "    # Đọc dữ liệu\n",
    "    data_train = pd.read_csv(os.path.join(folder_path, train_file))\n",
    "    data_test = pd.read_csv(os.path.join(folder_path, test_file))       \n",
    "\n",
    "    # Tách features và labels\n",
    "    X_train, y_train = data_train.drop([\"name\", \"bug\"], axis=1), data_train[\"bug\"]\n",
    "    X_test, y_test = data_test.drop([\"name\", \"bug\"], axis=1), data_test[\"bug\"]\n",
    "\n",
    "    # Chuyển đổi nhãn thành dạng nhị phân\n",
    "    y_train = y_train.apply(lambda x: 1 if x != 0 else 0)\n",
    "    y_test = y_test.apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_test = imputer.transform(X_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def Train(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Lựa chọn đặc trưng\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "    X_train = selector.fit_transform(X_train, y_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "\n",
    "    # Xử lý mất cân bằng lớp với Borderline-SMOTE\n",
    "    smote = BorderlineSMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),  # Thêm KNN\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),  # Thêm AdaBoost\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),  # Thêm XGBoost\n",
    "        ('lgbm', LGBMClassifier(random_state=42)),  # Thêm LightGBM\n",
    "        ('nb', GaussianNB())  # Thêm Naive Bayes\n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta (sử dụng Logistic Regression)\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    # Tính toán và trả về điểm F1\n",
    "    return f1_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Without_Feature_Selection(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Xử lý mất cân bằng lớp với Borderline-SMOTE\n",
    "    smote = BorderlineSMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)),\n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta (sử dụng Logistic Regression)\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    # Tính toán và trả về điểm F1\n",
    "    return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Without_Sampling(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Lựa chọn đặc trưng\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "    X_train = selector.fit_transform(X_train, y_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)), \n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')), \n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta (sử dụng Logistic Regression)\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    # Tính toán và trả về điểm F1\n",
    "    return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Without_Weighted_Learning(file_list, folder_path, index):\n",
    "    X_train, y_train, X_test, y_test = pre_processing(file_list, folder_path, index)\n",
    "\n",
    "    # Lựa chọn đặc trưng\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "    X_train = selector.fit_transform(X_train, y_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "\n",
    "    # Xử lý mất cân bằng lớp với Borderline-SMOTE\n",
    "    smote = BorderlineSMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Giảm chiều dữ liệu bằng PCA\n",
    "    pca = PCA(n_components=min(10, X_train.shape[1]), random_state=42)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()), \n",
    "        ('ada', AdaBoostClassifier(random_state=42)), \n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')), \n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    # Định nghĩa mô hình meta (sử dụng Logistic Regression)\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    # Huấn luyện mô hình stacking\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "    return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, train_test_split\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def Train_Cross(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    # Xử lý dữ liệu\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype == \"object\":\n",
    "            if col == \"bug\":\n",
    "                dataset[col] = dataset[col].astype(float)\n",
    "            else:\n",
    "                dataset = dataset.drop([col], axis=1)\n",
    "\n",
    "    X = dataset.drop([\"bug\"], axis=1)\n",
    "    y = dataset[\"bug\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    # Tạo pipeline cho việc tiền xử lý dữ liệu\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=mutual_info_classif)),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation với SMOTE chỉ áp dụng lên tập huấn luyện\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    \n",
    "    f1_scores = []  # Lưu trữ điểm F1 của mỗi lần phân chia\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Áp dụng SMOTE chỉ trên tập huấn luyện\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Biến đổi dữ liệu qua pipeline\n",
    "        X_train_transformed = pipeline.fit_transform(X_train_resampled, y_train_resampled)\n",
    "        X_test_transformed = pipeline.transform(X_test)  # Chỉ biến đổi dữ liệu test\n",
    "\n",
    "        # Định nghĩa các mô hình cơ sở\n",
    "        base_models = [\n",
    "            ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "            ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "            ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('knn', KNeighborsClassifier()),\n",
    "            ('ada', AdaBoostClassifier(random_state=42)),\n",
    "            ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "            ('lgbm', LGBMClassifier(random_state=42)),\n",
    "            ('nb', GaussianNB())\n",
    "        ]\n",
    "\n",
    "        meta_model = LogisticRegression()\n",
    "\n",
    "        # Sử dụng StackingClassifier\n",
    "        stacking_clf = StackingClassifier(\n",
    "            estimators=base_models, \n",
    "            final_estimator=meta_model,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Huấn luyện mô hình trên tập huấn luyện đã resample\n",
    "        stacking_clf.fit(X_train_transformed, y_train_resampled)\n",
    "\n",
    "        # Dự đoán trên tập kiểm tra\n",
    "        from sklearn.metrics import f1_score\n",
    "        y_pred = stacking_clf.predict(X_test_transformed)\n",
    "        \n",
    "        # Tính toán điểm F1 trên tập kiểm tra\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Trả về điểm F1 trung bình qua các vòng cross-validation\n",
    "    return sum(f1_scores) / len(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cross_Without_Feature_Selection(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype == \"object\":\n",
    "            if col == \"bug\":\n",
    "                dataset[col] = dataset[col].astype(float)\n",
    "            else:\n",
    "                dataset = dataset.drop([col], axis=1)\n",
    "    X = dataset.drop([\"bug\"], axis=1)\n",
    "    y = dataset[\"bug\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Tạo pipeline cho dữ liệu đã resample\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Biến đổi dữ liệu qua pipeline\n",
    "    X_transformed = pipeline.fit_transform(X_resampled, y_resampled)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB())\n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier \n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    scores = cross_val_score(stacking_clf, X_transformed, y_resampled, cv=kf, scoring='f1', n_jobs=-1)\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cross_Without_Sampling(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype == \"object\":\n",
    "            if col == \"bug\":\n",
    "                dataset[col] = dataset[col].astype(float)\n",
    "            else:\n",
    "                dataset = dataset.drop([col], axis=1)\n",
    "    X = dataset.drop([\"bug\"], axis=1)\n",
    "    y = dataset[\"bug\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "    \n",
    "    # Tạo pipeline cho dữ liệu đã resample\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=mutual_info_classif)),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Biến đổi dữ liệu qua pipeline\n",
    "    X_transformed = pipeline.fit_transform(X, y)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('ada', AdaBoostClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB())\n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier \n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    scores = cross_val_score(stacking_clf, X_transformed, y, cv=kf, scoring='f1', n_jobs=-1)\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Cross_Without_Weighted_Learning(file_list, folder_path):\n",
    "    data_file = file_list[0]\n",
    "    dataset = pd.read_csv(os.path.join(folder_path, data_file))\n",
    "\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype == \"object\":\n",
    "            if col == \"bug\":\n",
    "                dataset[col] = dataset[col].astype(float)\n",
    "            else:\n",
    "                dataset = dataset.drop([col], axis=1)\n",
    "    X = dataset.drop([\"bug\"], axis=1)\n",
    "    y = dataset[\"bug\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Tạo pipeline cho dữ liệu đã resample\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=mutual_info_classif)),\n",
    "        ('pca', PCA(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Biến đổi dữ liệu qua pipeline\n",
    "    X_transformed = pipeline.fit_transform(X_resampled, y_resampled)\n",
    "\n",
    "    base_models = [\n",
    "        ('logreg', LogisticRegression(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()), \n",
    "        ('ada', AdaBoostClassifier(random_state=42)), \n",
    "        ('xgb', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')), \n",
    "        ('lgbm', LGBMClassifier(random_state=42)), \n",
    "        ('nb', GaussianNB()) \n",
    "    ]\n",
    "\n",
    "    meta_model = LogisticRegression()\n",
    "\n",
    "    # Sử dụng StackingClassifier \n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models, \n",
    "        final_estimator=meta_model,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    scores = cross_val_score(stacking_clf, X_transformed, y_resampled, cv=kf, scoring='f1', n_jobs=-1)\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m     results_without_weighted_learning\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject\u001b[39m\u001b[38;5;124m\"\u001b[39m: project, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1_Score\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_score_without_weighted_learning})\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m     avg_score \u001b[38;5;241m=\u001b[39m \u001b[43mTrain_Cross\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     avg_score_without_feature_selection \u001b[38;5;241m=\u001b[39m Train_Cross_Without_Feature_Selection(versions, folder_path)\n\u001b[0;32m     30\u001b[0m     avg_score_without_sampling \u001b[38;5;241m=\u001b[39m Train_Cross_Without_Sampling(versions, folder_path)\n",
      "Cell \u001b[1;32mIn[9], line 78\u001b[0m, in \u001b[0;36mTrain_Cross\u001b[1;34m(file_list, folder_path)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Cross-validation\u001b[39;00m\n\u001b[0;32m     77\u001b[0m kf \u001b[38;5;241m=\u001b[39m RepeatedStratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacking_clf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Huấn luyện mô hình trên tập huấn luyện và tính toán điểm số trên tập kiểm tra\u001b[39;00m\n\u001b[0;32m     81\u001b[0m stacking_clf\u001b[38;5;241m.\u001b[39mfit(X_train_transformed, y_train_resampled)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "results = []\n",
    "results_without_feature_selection = []\n",
    "results_without_sampling = []\n",
    "results_without_weighted_learning = []\n",
    "for project, versions in projects.items():\n",
    "    project_scores = []\n",
    "    project_scores_without_feature_selection = []\n",
    "    project_scores_without_sampling = []\n",
    "    project_scores_without_weighted_learning = []\n",
    "    if len(versions) != 1:\n",
    "        for i in range(len(versions) - 1):\n",
    "            project_scores.append(Train(versions, folder_path, i))\n",
    "            project_scores_without_feature_selection.append(Train_Without_Feature_Selection(versions, folder_path, i))\n",
    "            project_scores_without_sampling.append(Train_Without_Sampling(versions, folder_path, i))\n",
    "            project_scores_without_weighted_learning.append(Train_Without_Weighted_Learning(versions, folder_path, i))\n",
    "\n",
    "        avg_score = np.mean(project_scores) \n",
    "        avg_score_without_feature_selection = np.mean(project_scores_without_feature_selection)\n",
    "        avg_score_without_sampling = np.mean(project_scores_without_sampling)\n",
    "        avg_score_without_weighted_learning = np.mean(project_scores_without_weighted_learning)\n",
    "\n",
    "        results.append({\"Project\": project, \"F1_Score\": avg_score})\n",
    "        results_without_feature_selection.append({\"Project\": project, \"F1_Score\": avg_score_without_feature_selection})\n",
    "        results_without_sampling.append({\"Project\": project, \"F1_Score\": avg_score_without_sampling})\n",
    "        results_without_weighted_learning.append({\"Project\": project, \"F1_Score\": avg_score_without_weighted_learning})\n",
    "    else:\n",
    "        avg_score = Train_Cross(versions, folder_path)\n",
    "        avg_score_without_feature_selection = Train_Cross_Without_Feature_Selection(versions, folder_path)\n",
    "        avg_score_without_sampling = Train_Cross_Without_Sampling(versions, folder_path)\n",
    "        avg_score_without_weighted_learning = Train_Cross_Without_Weighted_Learning(versions, folder_path)\n",
    "        \n",
    "        results.append({\"Project\": project, \"F1_Score\": avg_score})\n",
    "        results_without_feature_selection.append({\"Project\": project, \"F1_Score\": avg_score_without_feature_selection})\n",
    "        results_without_sampling.append({\"Project\": project, \"F1_Score\": avg_score_without_sampling})\n",
    "        results_without_weighted_learning.append({\"Project\": project, \"F1_Score\": avg_score_without_weighted_learning})\n",
    "\n",
    "avg_results = np.mean([result[\"F1_Score\"] for result in results])\n",
    "avg_results_without_feature_selection = np.mean([result[\"F1_Score\"] for result in results_without_feature_selection])\n",
    "avg_results_without_sampling = np.mean([result[\"F1_Score\"] for result in results_without_sampling])\n",
    "avg_results_without_weighted_learning = np.mean([result[\"F1_Score\"] for result in results_without_weighted_learning])\n",
    "\n",
    "results.append({\"Project\": \"Avg\", \"F1_Score\": avg_results})\n",
    "results_without_feature_selection.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_feature_selection})\n",
    "results_without_sampling.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_sampling})\n",
    "results_without_weighted_learning.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_weighted_learning})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results_without_feature_selection = pd.DataFrame(results_without_feature_selection)\n",
    "df_results_without_sampling = pd.DataFrame(results_without_sampling)\n",
    "df_results_without_weighted_learning = pd.DataFrame(results_without_weighted_learning)\n",
    "\n",
    "output_file = 'dataPROMISE_results.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_results.to_excel(writer, sheet_name='Results', index=False)\n",
    "    df_results_without_feature_selection.to_excel(writer, sheet_name='Results_Without_Feature_Selection', index=False)\n",
    "    df_results_without_sampling.to_excel(writer, sheet_name='Results_Without_Sampling', index=False)\n",
    "    df_results_without_weighted_learning.to_excel(writer, sheet_name='Results_Without_Weighted_Learning', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CM1.csv', 'KC1.csv', 'KC3.csv', 'MC1.csv', 'MC2.csv', 'MW1.csv', 'PC1.csv', 'PC3.csv', 'PC4.csv', 'PC5.csv']\n"
     ]
    }
   ],
   "source": [
    "folder_path_nasa = \"dataNASA/\"\n",
    "files_nasa = [f for f in os.listdir(folder_path_nasa) if f.endswith('.csv')]\n",
    "ant_files_nasa = sorted([f for f in files if f.startswith('')])\n",
    "file_list_nasa = os.listdir(folder_path_nasa)\n",
    "print(file_list_nasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results_without_feature_selection = []\n",
    "results_without_sampling = []\n",
    "results_without_weighted_learning = []\n",
    "for file in file_list_nasa:\n",
    "        avg_score = Train_Cross(versions, folder_path)\n",
    "        avg_score_without_feature_selection = Train_Cross_Without_Feature_Selection(versions, folder_path)\n",
    "        avg_score_without_sampling = Train_Cross_Without_Sampling(versions, folder_path)\n",
    "        avg_score_without_weighted_learning = Train_Cross_Without_Weighted_Learning(versions, folder_path)\n",
    "        \n",
    "        results.append({\"Project\": file, \"F1_Score\": avg_score})\n",
    "        results_without_feature_selection.append({\"Project\": file, \"F1_Score\": avg_score_without_feature_selection})\n",
    "        results_without_sampling.append({\"Project\": file, \"F1_Score\": avg_score_without_sampling})\n",
    "        results_without_weighted_learning.append({\"Project\": file, \"F1_Score\": avg_score_without_weighted_learning})\n",
    "\n",
    "avg_results = np.mean([result[\"F1_Score\"] for result in results])\n",
    "avg_results_without_feature_selection = np.mean([result[\"F1_Score\"] for result in results_without_feature_selection])\n",
    "avg_results_without_sampling = np.mean([result[\"F1_Score\"] for result in results_without_sampling])\n",
    "avg_results_without_weighted_learning = np.mean([result[\"F1_Score\"] for result in results_without_weighted_learning])\n",
    "\n",
    "results.append({\"Project\": \"Avg\", \"F1_Score\": avg_results})\n",
    "results_without_feature_selection.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_feature_selection})\n",
    "results_without_sampling.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_sampling})\n",
    "results_without_weighted_learning.append({\"Project\": \"Avg\", \"F1_Score\": avg_results_without_weighted_learning})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results_without_feature_selection = pd.DataFrame(results_without_feature_selection)\n",
    "df_results_without_sampling = pd.DataFrame(results_without_sampling)\n",
    "df_results_without_weighted_learning = pd.DataFrame(results_without_weighted_learning)\n",
    "\n",
    "output_file = 'dataNASA_results.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_results.to_excel(writer, sheet_name='Results', index=False)\n",
    "    df_results_without_feature_selection.to_excel(writer, sheet_name='Results_Without_Feature_Selection', index=False)\n",
    "    df_results_without_sampling.to_excel(writer, sheet_name='Results_Without_Sampling', index=False)\n",
    "    df_results_without_weighted_learning.to_excel(writer, sheet_name='Results_Without_Weighted_Learning', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
